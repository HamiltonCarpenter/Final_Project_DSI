{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk.data\n",
    "from collections import Counter\n",
    "import time\n",
    "import pickle\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from pymongo import MongoClient, errors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mall-the-news\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/all-the-news/articles1.csv')\n",
    "df2 = pd.read_csv('data/all-the-news/articles2.csv')\n",
    "df3 = pd.read_csv('data/all-the-news/articles3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1pub = df1['publication']\n",
    "df2pub = df2['publication']\n",
    "df3pub = df3['publication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Breitbart           23781\n",
       "CNN                 11488\n",
       "New York Times       7803\n",
       "Business Insider     6757\n",
       "Atlantic              171\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1pub.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "New York Post          17493\n",
       "Atlantic                7008\n",
       "National Review         6203\n",
       "Talking Points Memo     5214\n",
       "Guardian                4873\n",
       "Buzzfeed News           4854\n",
       "Fox News                4354\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2pub.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NPR                11992\n",
       "Washington Post    11114\n",
       "Reuters            10710\n",
       "Vox                 4947\n",
       "Guardian            3808\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3pub.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vox = 4947\n",
    "#Buzzfeed_News = 854\n",
    "the_atlantic = 7171\n",
    "CNN = 11488\n",
    "BuzzFeedNEWS = 4854\n",
    "\n",
    "\n",
    "#Fox_news = 4354\n",
    "National_Review = 6203\n",
    "Breitbart = 23781\n",
    "New_York_Post = 17493\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23606"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vox + the_atlantic +CNN#+BuzzFeedNEWS #Buzzfeed_News + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47477"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "National_Review +Breitbart +New_York_Post #Fox_news + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_join = df1 +df2+df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Breitbart_Articles = (df1[df1[\"publication\"]==\"Breitbart\"])\n",
    "National_Review_Articles = (df2[df2[\"publication\"]==\"National Review\"])\n",
    "New_York_Post_Articles = (df2[df2[\"publication\"]==\"New York Post\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vox_Articles = (df3[df3[\"publication\"]==\"Vox\"])\n",
    "Atlantic_Articles1 = (df1[df1[\"publication\"]==\"Atlantic\"])\n",
    "Atlantic_Articles = Atlantic_Articles1.append((df2[df2[\"publication\"]==\"Atlantic\"]))\n",
    "CNN_Articles = (df1[df1[\"publication\"]==\"CNN\"])\n",
    "#Buzzfeed_Articles = (df2[df2[\"publication\"]==\"Buzzfeed News\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conservative_Pubs = Breitbart_Articles\n",
    "Conservative_Pubs = Conservative_Pubs.append(National_Review_Articles)\n",
    "Conservative_Pubs = Conservative_Pubs.append(New_York_Post_Articles)\n",
    "\n",
    "Liberal_Pubs = Vox_Articles\n",
    "Liberal_Pubs = Liberal_Pubs.append(Atlantic_Articles)\n",
    "Liberal_Pubs = Liberal_Pubs.append(CNN_Articles)\n",
    "\n",
    "x=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23614, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Liberal_Pubs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47477, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conservative_Pubs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_eval = Conservative_Pubs['publication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Breitbart          23781\n",
       "New York Post      17493\n",
       "National Review     6203\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_eval.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_text = Conservative_Pubs['content']\n",
    "lib_text = Liberal_Pubs['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'title', 'publication', 'author', 'date', 'year',\n",
       "       'month', 'url', 'content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conservative_Pubs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23614"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lib_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done  0  out of  23614\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "descriptor 'split' requires a 'str' object but received a 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-69b871e6946a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" out of \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlib_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: descriptor 'split' requires a 'str' object but received a 'list'"
     ]
    }
   ],
   "source": [
    "lib_words = {}\n",
    "for i in range(0,len(lib_text)):\n",
    "    if i%100 ==0:\n",
    "        print(\"done \",i,\" out of \",len(lib_text))\n",
    "    lib_text[i] = str.split(lib_text[i])\n",
    "for i in range(0,len(con_text)):\n",
    "    if i%100 ==0:\n",
    "        print(\"done \",i,\" out of \",len(lib_text))\n",
    "    con_text[i] = str.split(con_text[i])\n",
    "\n",
    "for doc in lib_text.head():\n",
    "    for word in doc:\n",
    "        if word in lib_words:\n",
    "            lib_words[word] = lib_words[word] +1\n",
    "        else:\n",
    "            lib_words[word] = 1\n",
    "\n",
    "            \n",
    "#for doc in con_text.head():\n",
    "#    for word in doc:\n",
    "#        if word in con_words:\n",
    " #           con_words[word] = con_words[word] +1\n",
    " #       else:\n",
    "  #          con_words[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "docs = [word_tokenize(str(content)) for content in con_text]\n",
    "\n",
    "# Remove stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "docs = [[word for word in words if ((word not in stop)and (word not in string.punctuation))] for words in docs]\n",
    "\n",
    "# lemmatize\n",
    "wordnet = WordNetLemmatizer()\n",
    "docs_wordnet = [[wordnet.lemmatize(word) for word in words] for words in docs]\n",
    "words_rucksack = []\n",
    "for doc in docs_wordnet:\n",
    "    for word in doc:\n",
    "        words_rucksack.append(word)\n",
    "con_cnt = Counter(words_rucksack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "docs = [word_tokenize(str(content)) for content in lib_text]\n",
    "\n",
    "# Remove stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "docs = [[word for word in words if ((word not in stop)and (word not in string.punctuation))] for words in docs]\n",
    "\n",
    "# lemmatize\n",
    "wordnet = WordNetLemmatizer()\n",
    "docs_wordnet = [[wordnet.lemmatize(word) for word in words] for words in docs]\n",
    "words_rucksack = []\n",
    "for doc in docs_wordnet:\n",
    "    for word in doc:\n",
    "        words_rucksack.append(word)\n",
    "cnt = Counter(words_rucksack)\n",
    "#words_rucksack = [word for word in doc for doc in docs_wordnet]\n",
    "#docs_wordnet = [(wordnet.lemmatize(word) for word in words) for words in docs]\n",
    "#for doc in docs_wordnet:\n",
    "    #for word in doc:\n",
    "#    if word in lib_words:\n",
    "#        lib_words[word] = lib_words[word] +1\n",
    "#    else:\n",
    "#        lib_words[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for content in lib_text:\n",
    "    i=i+1\n",
    "    word_tokenize(str(content))\n",
    "    if i == 5000:\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197285"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(con_cnt,open('con_count.p','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23614"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lib_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_c = pickle.load(open('con_count.p','rb'))\n",
    "lib_c = pickle.load(open('lib_count.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    con_c = pickle.load(open('con_count.p','rb'))\n",
    "    lib_c = pickle.load(open('lib_count.p','rb'))\n",
    "    array_words = ''.split(text)\n",
    "    lib_strength = 0\n",
    "    con_strength = 0\n",
    "    for word in array_words:\n",
    "        if word in con_c:\n",
    "            con_tf = con_c[word]\n",
    "        else:\n",
    "            con_tf = 1\n",
    "        if word in lib_c:\n",
    "            lib_tf = lib_c[word]\n",
    "        else:\n",
    "            lib_tf = 1\n",
    "        lib_strength = lib_strength + (lib_tf/(lib_tf+con_tf))\n",
    "        con_strength = con_strength + (con_tf/(lib_tf+con_tf))\n",
    "    return lib_strength, con_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FirstModel.ipynb  Untitled.ipynb    \u001b[1m\u001b[36mdata\u001b[m\u001b[m/             \u001b[1m\u001b[36msubmissions\u001b[m\u001b[m/\n",
      "README.md         assessment1.md    \u001b[1m\u001b[36mimages\u001b[m\u001b[m/           topics.md\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

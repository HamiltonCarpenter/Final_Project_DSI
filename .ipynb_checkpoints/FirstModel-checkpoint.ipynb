{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk.data\n",
    "from collections import Counter\n",
    "import time\n",
    "import pickle\n",
    "import timeit\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from pymongo import MongoClient, errors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: url, dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       ...,\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
       "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
       "2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "\n",
       "      publication                         author        date    year  month  \\\n",
       "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
       "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
       "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
       "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
       "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
       "\n",
       "   url                                            content  \n",
       "0  NaN  WASHINGTON  —   Congressional Republicans have...  \n",
       "1  NaN  After the bullet shells get counted, the blood...  \n",
       "2  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
       "3  NaN  Death may be the great equalizer, but it isn’t...  \n",
       "4  NaN  SEOUL, South Korea  —   North Korea’s leader, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mall-the-news\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/all-the-news/articles1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/all-the-news/articles2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('data/all-the-news/articles3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1pub = df1['publication']\n",
    "df2pub = df2['publication']\n",
    "df3pub = df3['publication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Breitbart           23781\n",
       "CNN                 11488\n",
       "New York Times       7803\n",
       "Business Insider     6757\n",
       "Atlantic              171\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1pub.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "New York Post          17493\n",
       "Atlantic                7008\n",
       "National Review         6203\n",
       "Talking Points Memo     5214\n",
       "Guardian                4873\n",
       "Buzzfeed News           4854\n",
       "Fox News                4354\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2pub.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NPR                11992\n",
       "Washington Post    11114\n",
       "Reuters            10710\n",
       "Vox                 4947\n",
       "Guardian            3808\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3pub.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vox = 4947\n",
    "#Buzzfeed_News = 854\n",
    "the_atlantic = 7171\n",
    "CNN = 11488\n",
    "BuzzFeedNEWS = 4854\n",
    "\n",
    "\n",
    "#Fox_news = 4354\n",
    "National_Review = 6203\n",
    "Breitbart = 23781\n",
    "New_York_Post = 17493\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23606"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vox + the_atlantic +CNN#+BuzzFeedNEWS #Buzzfeed_News + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47477"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "National_Review +Breitbart +New_York_Post #Fox_news + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On Tuesday’s broadcast of CNN’s “Situation Room,” CNN Senior Washington Correspondent Jeff Zeleny stated that Chelsea Manning’s transition from a man to a woman “certainly played into” President Obama’s decision to commute Manning’s sentence, and “Without that, it’s hard to imagine, I think, this president would have done that. ”  Zeleny said, “I think a question that this president the White House will have to answer here  —   will answer, and I think it’s an important one, if  —   how much was the personal story of Chelsea Manning involved in this, because the outcry from the left was so strong on this. And she’s having a difficult time in federal prison, no question. But, to me, that is a central question here. Without that, you have to wonder if the outcome would be the same. I think it might not be. ” He added, “[B]ecause she transitioned from a man to a woman, I think all of that certainly played into this. Without that, it’s hard to imagine, I think, this president would have done that. ” (  Mediaite) Follow Ian Hanchett on Twitter @IanHanchett'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Breitbart_Articles['content'][7803]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7803    On Tuesday’s broadcast of CNN’s “Situation Roo...\n",
       "7804    A group of American spring break revelers repo...\n",
       "7805      “honour crimes” have risen by 40 per cent in...\n",
       "7806    Former Breitbart Senior Editor MILO has announ...\n",
       "7807    The focus of the continuous media reports of a...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Breitbart_Articles['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-05e098bc57cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBreitbart_Articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"publication\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"Breitbart\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mNational_Review_Articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"publication\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"National Review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mNew_York_Post_Articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"publication\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"New York Post\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "Breitbart_Articles = (df1[df1[\"publication\"]==\"Breitbart\"])\n",
    "National_Review_Articles = (df2[df2[\"publication\"]==\"National Review\"])\n",
    "New_York_Post_Articles = (df2[df2[\"publication\"]==\"New York Post\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vox_Articles = (df3[df3[\"publication\"]==\"Vox\"])\n",
    "Atlantic_Articles1 = (df1[df1[\"publication\"]==\"Atlantic\"])\n",
    "Atlantic_Articles = Atlantic_Articles1.append((df2[df2[\"publication\"]==\"Atlantic\"]))\n",
    "CNN_Articles = (df1[df1[\"publication\"]==\"CNN\"])\n",
    "#Buzzfeed_Articles = (df2[df2[\"publication\"]==\"Buzzfeed News\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buzzfeed_Articles = (df2[df2[\"publication\"]==\"Buzzfeed News\"])\n",
    "Fox_Articles = (df2[df2[\"publication\"]==\"Fox News\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conservative_Pubs = Breitbart_Articles\n",
    "Conservative_Pubs = Conservative_Pubs.append(National_Review_Articles)\n",
    "Conservative_Pubs = Conservative_Pubs.append(New_York_Post_Articles)\n",
    "\n",
    "Liberal_Pubs = Vox_Articles\n",
    "Liberal_Pubs = Liberal_Pubs.append(Atlantic_Articles)\n",
    "Liberal_Pubs = Liberal_Pubs.append(CNN_Articles)\n",
    "\n",
    "x=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23614, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Liberal_Pubs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47477, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conservative_Pubs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_eval = Conservative_Pubs['publication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Breitbart          23781\n",
       "New York Post      17493\n",
       "National Review     6203\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_eval.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_text = Conservative_Pubs['content']\n",
    "lib_text = Liberal_Pubs['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'title', 'publication', 'author', 'date', 'year',\n",
       "       'month', 'url', 'content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conservative_Pubs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23614"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lib_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done  0  out of  23614\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "descriptor 'split' requires a 'str' object but received a 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-69b871e6946a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" out of \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlib_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: descriptor 'split' requires a 'str' object but received a 'list'"
     ]
    }
   ],
   "source": [
    "lib_words = {}\n",
    "for i in range(0,len(lib_text)):\n",
    "    if i%100 ==0:\n",
    "        print(\"done \",i,\" out of \",len(lib_text))\n",
    "    lib_text[i] = str.split(lib_text[i])\n",
    "for i in range(0,len(con_text)):\n",
    "    if i%100 ==0:\n",
    "        print(\"done \",i,\" out of \",len(lib_text))\n",
    "    con_text[i] = str.split(con_text[i])\n",
    "\n",
    "for doc in lib_text.head():\n",
    "    for word in doc:\n",
    "        if word in lib_words:\n",
    "            lib_words[word] = lib_words[word] +1\n",
    "        else:\n",
    "            lib_words[word] = 1\n",
    "\n",
    "            \n",
    "#for doc in con_text.head():\n",
    "#    for word in doc:\n",
    "#        if word in con_words:\n",
    " #           con_words[word] = con_words[word] +1\n",
    " #       else:\n",
    "  #          con_words[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "docs = [word_tokenize(str(content)) for content in con_text]\n",
    "\n",
    "# Remove stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "docs = [[word for word in words if ((word not in stop)and (word not in string.punctuation))] for words in docs]\n",
    "\n",
    "# lemmatize\n",
    "wordnet = WordNetLemmatizer()\n",
    "docs_wordnet = [[wordnet.lemmatize(word) for word in words] for words in docs]\n",
    "words_rucksack = []\n",
    "global_con_cnt = Counter()\n",
    "for doc in docs_wordnet:\n",
    "    local_con_list = []\n",
    "    for word in doc:\n",
    "        if word not in local_con_list:\n",
    "            local_con_list.append(word)\n",
    "    local_count = Counter(local_con_list)\n",
    "    global_con_cnt = global_con_cnt + local_count\n",
    "    \n",
    "    #for word in doc:\n",
    "        #words_rucksack.append(word)\n",
    "con_cnt = Counter(words_rucksack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 4, 'b': 3, 'c': 1})"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(a=3, b=1)\n",
    "d = Counter(a=1, b=2, c=1)\n",
    "c + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "docs = [word_tokenize(str(content)) for content in lib_text]\n",
    "\n",
    "# Remove stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "docs = [[word for word in words if ((word not in stop)and (word not in string.punctuation))] for words in docs]\n",
    "\n",
    "# lemmatize\n",
    "wordnet = WordNetLemmatizer()\n",
    "docs_wordnet = [[wordnet.lemmatize(word) for word in words] for words in docs]\n",
    "words_rucksack = []\n",
    "for doc in docs_wordnet:\n",
    "    for word in doc:\n",
    "        words_rucksack.append(word)\n",
    "cnt = Counter(words_rucksack)\n",
    "#words_rucksack = [word for word in doc for doc in docs_wordnet]\n",
    "#docs_wordnet = [(wordnet.lemmatize(word) for word in words) for words in docs]\n",
    "#for doc in docs_wordnet:\n",
    "    #for word in doc:\n",
    "#    if word in lib_words:\n",
    "#        lib_words[word] = lib_words[word] +1\n",
    "#    else:\n",
    "#        lib_words[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for content in lib_text:\n",
    "    i=i+1\n",
    "    word_tokenize(str(content))\n",
    "    if i == 5000:\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(con_text)\n",
    "pickle.dump(len(con_text),open('con_doc_count.p','wb'))\n",
    "pickle.dump(len(lib_text),open('lib_doc_count.p','wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(con_cnt,open('con_count.p','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23614"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lib_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_words = 0\n",
    "for doc in lib_text:\n",
    "    lib_words = lib_words + len(word_tokenize(str(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_words = 0\n",
    "for doc in con_text:\n",
    "    con_words = con_words + len(word_tokenize(str(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30006946"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_text = pd.DataFrame(con_text).reset_index(drop=True)\n",
    "con_text.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_text = con_text['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_c = pickle.load(open('con_count.p','rb'))\n",
    "lib_c = pickle.load(open('lib_count.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "C_lib_test_articles = 0\n",
    "C_con_test_articles = 0\n",
    "C_zero_articles = 0\n",
    "\n",
    "C_lib_test_articles_strength = 0\n",
    "C_con_test_articles_strength = 0\n",
    "\n",
    "i = 0\n",
    "for article in Fox_Articles['content']:\n",
    "    if i < 6000:\n",
    "        l,c = classify(article)\n",
    "        if (l>c):\n",
    "            C_lib_test_articles = C_lib_test_articles +1\n",
    "            C_lib_test_articles_strength = C_lib_test_articles_strength +1 * l\n",
    "        elif(c>l):\n",
    "            C_con_test_articles = C_con_test_articles +1\n",
    "            C_con_test_articles_strength = C_con_test_articles_strength +1 * c\n",
    "        else:\n",
    "            C_zero_articles = C_zero_articles + 1\n",
    "        i = i + 1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_lib_test_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "923"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_con_test_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4354,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fox_Articles['content'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_lib_test_articles = 0\n",
    "L_con_test_articles = 0\n",
    "L_zero_articles = 0\n",
    "\n",
    "L_lib_test_articles_strength = 0\n",
    "L_con_test_articles_strength = 0\n",
    "\n",
    "i = 0\n",
    "for article in Buzzfeed_Articles['content']:\n",
    "    if i < 6000:\n",
    "        l,c = classify(article)\n",
    "        if (l>c):\n",
    "            L_lib_test_articles = L_lib_test_articles +1\n",
    "            L_lib_test_articles_strength = L_lib_test_articles_strength +1 * l\n",
    "        elif(c>l):\n",
    "            L_con_test_articles = L_con_test_articles +1\n",
    "            L_con_test_articles_strength = L_con_test_articles_strength +1 *c\n",
    "        else:\n",
    "            L_zero_articles = L_zero_articles + 1\n",
    "        i = i + 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4854, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Buzzfeed_Articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_lib_test_articles:  142\n",
      "L_lib_test_articles_strength:  2.826863900798512e-06\n",
      "L_con_test_articles:  825\n",
      "L_con_test_articles_strength:  0.2636346231712514\n",
      "L_zero_articles:  33\n"
     ]
    }
   ],
   "source": [
    "print('L_lib_test_articles: ',L_lib_test_articles)\n",
    "print('L_lib_test_articles_strength: ',L_lib_test_articles_strength)\n",
    "print('L_con_test_articles: ',L_con_test_articles)\n",
    "print('L_con_test_articles_strength: ',L_con_test_articles_strength)\n",
    "print('L_zero_articles: ',L_zero_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_lib_test_articles:  75\n",
      "C_lib_test_articles_strength:  1.066738907674856e-07\n",
      "C_con_test_articles:  923\n",
      "C_con_test_articles_strength:  0.31382835999574804\n",
      "C_zero_articles:  2\n"
     ]
    }
   ],
   "source": [
    "print('C_lib_test_articles: ',C_lib_test_articles)\n",
    "print('C_lib_test_articles_strength: ',C_lib_test_articles_strength)\n",
    "print('C_con_test_articles: ',C_con_test_articles)\n",
    "print('C_con_test_articles_strength: ',C_con_test_articles_strength)\n",
    "print('C_zero_articles: ',C_zero_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "825"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_con_test_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_zero_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    text = str(text)\n",
    "    con_c = pickle.load(open('con_count.p','rb'))\n",
    "    lib_c = pickle.load(open('lib_count.p','rb'))\n",
    "    #con_c = pickle.load(open('con_doc_count.p','rb'))\n",
    "    #lib_c = pickle.load(open('lib_doc_count.p','rb'))\n",
    "    array_words = text.split()\n",
    "    lib_strength = 1\n",
    "    con_strength = 1\n",
    "########### tokenize\n",
    "    token_text = word_tokenize(str(text))\n",
    "\n",
    "# Remove stopwords\n",
    "    stop = set(stopwords.words('english'))\n",
    "    doc = [word for word in token_text if ((word not in stop)and (word not in string.punctuation))]\n",
    "\n",
    "# lemmatize\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    docs_wordnet = [wordnet.lemmatize(word) for word in doc]\n",
    "    #print(str(docs_wordnet))\n",
    "    words_rucksack = []\n",
    "############\n",
    "    for word in docs_wordnet:\n",
    "        #print('word: ',word)\n",
    "        if word in con_c:\n",
    "            con_tf = con_c[word]\n",
    "        else:\n",
    "            con_tf = 1\n",
    "        if word in lib_c:\n",
    "            lib_tf = lib_c[word]\n",
    "        else:\n",
    "            lib_tf = 1\n",
    "        #print('--------------------------')\n",
    "        #print('term: ',word)\n",
    "        #print('con count: ',con_tf)\n",
    "        #print('lib count: ',lib_tf)    \n",
    "        #print('prior con s: ',con_strength) \n",
    "        #print('prior lib s: ',lib_strength) \n",
    "        if (con_tf > 1) or (lib_tf > 1):\n",
    "            lib_strength = lib_strength * np.sqrt(lib_tf/(lib_tf+con_tf))#devide by length of lib corpus\n",
    "            con_strength = con_strength * np.sqrt(con_tf/(lib_tf+con_tf))\n",
    "        #print('post con s: ',con_strength) \n",
    "        #print('post lib s: ',lib_strength)\n",
    "    return lib_strength, con_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l,c = classify(lib_text[26511])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26510     People have dreamed about flying cars for dec...\n",
       "26511     US intelligence agencies believe that this ye...\n",
       "26512     Minnesota Rep. Keith Ellison hadn’t been camp...\n",
       "26513     One of the biggest and most important science...\n",
       "26514     I gave up drinking when my pancreas exploded....\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_cost(pred,expected,value):\n",
    "    return abs(pred[value]-expected[value])\n",
    "    \n",
    "def evaluation(pred,expected):\n",
    "    error = 0\n",
    "    for outlet in pred:\n",
    "        error = error + order_cost(pred,expected,outlet)\n",
    "    \n",
    "#def test(pred,expected,value):\n",
    "    \n",
    "        \n",
    "        \n",
    "        #scheme 1: classification left or right\n",
    "        #scheme 2: order\n",
    "        #scheme 3: how close to target \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0060765049821886e-121"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.243571400758148e-118"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l,c = classify(test_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2340.1812895233998"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2429.8187104766025"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    WASHINGTON  —   Congressional Republicans have...\n",
       "1    After the bullet shells get counted, the blood...\n",
       "2    When Walt Disney’s “Bambi” opened in 1942, cri...\n",
       "3    Death may be the great equalizer, but it isn’t...\n",
       "4    SEOUL, South Korea  —   North Korea’s leader, ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lib_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all docs vs use strong docs with weight vs "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

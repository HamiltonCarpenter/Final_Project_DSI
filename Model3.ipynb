{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF as NMF_sklearn\n",
    "import pickle\n",
    "from nmf import NMF\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pickle\n",
    "def build_text_vectorizer(contents, use_tfidf=True, use_stemmer=False, max_features=None):\n",
    "    '''\n",
    "    Build and return a **callable** for transforming text documents to vectors,\n",
    "    as well as a vocabulary to map document-vector indices to words from the\n",
    "    corpus. The vectorizer will be trained from the text documents in the\n",
    "    `contents` argument. If `use_tfidf` is True, then the vectorizer will use\n",
    "    the Tf-Idf algorithm, otherwise a Bag-of-Words vectorizer will be used.\n",
    "    The text will be tokenized by words, and each word will be stemmed iff\n",
    "    `use_stemmer` is True. If `max_features` is not None, then the vocabulary\n",
    "    will be limited to the `max_features` most common words in the corpus.\n",
    "    '''\n",
    "    \n",
    "    Vectorizer = TfidfVectorizer if use_tfidf else CountVectorizer\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']+\")\n",
    "    stem = PorterStemmer().stem if use_stemmer else (lambda x: x)\n",
    "    stop_set = set(stopwords.words('english'))\n",
    "\n",
    "    # Closure over the tokenizer et al.\n",
    "    def tokenize(text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        stems = [stem(token) for token in tokens if token not in stop_set]\n",
    "        return stems\n",
    "\n",
    "    vectorizer_model = Vectorizer(tokenizer=tokenize, max_features=max_features)\n",
    "    vectorizer_model.fit(contents)\n",
    "    vocabulary = np.array(vectorizer_model.get_feature_names())\n",
    "\n",
    "    # Closure over the vectorizer_model's transform method.\n",
    "    def vectorizer(X):\n",
    "        return vectorizer_model.transform(X).toarray()\n",
    "\n",
    "    return vectorizer, vocabulary\n",
    "\n",
    "\n",
    "def softmax(v, temperature=1.0):\n",
    "    '''\n",
    "    A heuristic to convert arbitrary positive values into probabilities.\n",
    "    See: https://en.wikipedia.org/wiki/Softmax_function\n",
    "    '''\n",
    "    expv = np.exp(v / temperature)\n",
    "    s = np.sum(expv)\n",
    "    return expv / s\n",
    "\n",
    "\n",
    "def hand_label_topics(H, vocabulary):\n",
    "    '''\n",
    "    Print the most influential words of each latent topic, and prompt the user\n",
    "    to label each topic. The user should use their humanness to figure out what\n",
    "    each latent topic is capturing.\n",
    "    '''\n",
    "    hand_labels = []\n",
    "    for i, row in enumerate(H):\n",
    "        top_five = np.argsort(row)[::-1][:20]\n",
    "        print('topic', i)\n",
    "        print('-->', ' '.join(vocabulary[top_five]))\n",
    "        label = input('please label this topic: ')\n",
    "        hand_labels.append(label)\n",
    "        print()\n",
    "    return hand_labels\n",
    "\n",
    "\n",
    "def analyze_article(article_index, contents, web_urls, W, hand_labels):\n",
    "    '''\n",
    "    Print an analysis of a single NYT articles, including the article text\n",
    "    and a summary of which topics it represents. The topics are identified\n",
    "    via the hand-labels which were assigned by the user.\n",
    "    '''\n",
    "    #print(web_urls[article_index])\n",
    "    #print(contents[article_index])\n",
    "    probs = softmax(W[article_index], temperature=0.01)\n",
    "    \n",
    "    top_prob = 0\n",
    "    top_cat = 0\n",
    "    i = 0\n",
    "    for prob, label in zip(probs, hand_labels):\n",
    "        if prob > top_prob:\n",
    "            top_prob = prob\n",
    "            top_cat = i\n",
    "        #print('--> {:.2f}% {}'.format(prob * 100, label))\n",
    "        i = i + 1\n",
    "    return top_cat, probs\n",
    "    #print()\n",
    "    #    gotta assign all of these to the correct bin and then tfidf them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_nmf(heldout_name,train_data):\n",
    "    #make_names:\n",
    "    vectorizer_file_name = 'cache/'+heldout_name+' vectorizer'\n",
    "    vocabulary_file_name = 'cache/'+heldout_name+' vocabulary'\n",
    "    nmf_file_name = 'cache/'+heldout_name+' nmf'\n",
    "    \n",
    "    g_df = train_data \n",
    "    g_contents = g_df.content\n",
    "    g_web_urls = g_df.url\n",
    "\n",
    "    # Build our text-to-vector vectorizer, then vectorize our corpus.\n",
    "    g_vectorizer, g_vocabulary = build_text_vectorizer(g_contents,\n",
    "                                 use_tfidf=True,\n",
    "                                 use_stemmer=False,\n",
    "                                 max_features=5000)\n",
    "    g_X = g_vectorizer(g_contents)\n",
    "\n",
    "    # We'd like to see consistent results, so set the seed.\n",
    "    np.random.seed(12345)\n",
    "\n",
    "    #g_rand_articles = list(range(len(g_df_full)))\n",
    "    #for i in rand_articles:\n",
    "    #        analyze_article(i, contents, web_urls, W, hand_labels)\n",
    "\n",
    "    # Do it all again, this time using scikit-learn.\n",
    "    g_nmf = NMF_sklearn(n_components=6, max_iter=100, random_state=12345, alpha=0.0)\n",
    "    g_W = g_nmf.fit_transform(g_X)\n",
    "    g_H = g_nmf.components_\n",
    "    print('reconstruction error:', g_nmf.reconstruction_err_)\n",
    "    #g_hand_labels = ['label_one', 'label_two', 'label_three', 'label_four', 'label_five', 'label_one'] # hand_label_topics(g_H, g_vocabulary) #\n",
    "    \n",
    "    #pickle.dump(g_vectorizer,open(vectorizer_file_name,'wb'))\n",
    "    #pickle.dump(g_vocabulary,open(vocabulary_file_name,'wb'))\n",
    "    #pickle.dump(g_nmf,open(nmf_file_name,'wb'))\n",
    "    \n",
    "    return g_vectorizer, g_vocabulary, g_nmf\n",
    "#pickle.dump(H,open('nyt_clusters_H.p','wb'))\n",
    "#pickle.dump(W,open('nyt_clusters_W.p','wb'))\n",
    "\n",
    "def train_random_forest(cleaned_nmf_compenents, raw_training_data, cleaned_train_y,trees_in_forest = 100):\n",
    "    print('trees in this forest: ', trees_in_forest)\n",
    "    #outlets = raw_training_data['publication']\n",
    "    #y_train = pd.Series([outlet_leaning[outlet] for outlet in outlets])\n",
    "    y_train = cleaned_train_y\n",
    "    X_train = cleaned_nmf_compenents\n",
    "    rf = RandomForestClassifier(n_estimators=trees_in_forest, oob_score=True)\n",
    "    rf.fit(X_train, y_train)\n",
    "    #print(\"\\n11: accuracy score:\", rf.score(X_test, y_test))\n",
    "    #print(\"    out of bag score:\", rf.oob_score_)\n",
    "    return rf\n",
    "\n",
    "def read_in_raw_data():\n",
    "    g_df1 = pd.read_csv('data/all-the-news/articles1.csv')\n",
    "    g_df2 = pd.read_csv('data/all-the-news/articles2.csv')\n",
    "    g_df3 = pd.read_csv('data/all-the-news/articles3.csv')\n",
    "\n",
    "    g_arr1 = g_df1.values\n",
    "    g_arr2 = g_df2.values\n",
    "    g_arr3 = g_df3.values\n",
    "\n",
    "    g_df_full = g_df1.append(g_df2).append(g_df3)\n",
    "    g_df_reset=g_df_full.reset_index(drop=True)\n",
    "    g_df_reset.head()\n",
    "    g_df_reset.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    g_df_full = g_df_reset\n",
    "    return g_df_full\n",
    "    \n",
    "def get_test_train(heldout,data):\n",
    "    return data[(data['publication'] != heldout)], data[(data['publication'] == heldout)]\n",
    "    \n",
    "def predictions():\n",
    "    pass\n",
    "\n",
    "    \n",
    "def evaluate_model(vectorizer, vocabulary, nmf, heldout_outlet, test_data,outlets):\n",
    "    train_data, test_data = get_test_train(heldout,data)\n",
    "    \n",
    "    \n",
    "#maybe remove wapo\n",
    "def kfolds():\n",
    "    #outlets =[('Fox News',1),('National Review',1),('National Review',1),('New York Post',1),('Breitbart',1),\n",
    "    #          ('Buzzfeed News',0),('Vox',0)('Atlantic',0),('Washington Post',0),('CNN',0)]\n",
    "    outlets =['Fox News','National Review','National Review','New York Post','Breitbart',\n",
    "              'Buzzfeed News','Vox','Atlantic','Washington Post','CNN']\n",
    "    leanings_dict = {'Fox News':1,'National Review':1,'National Review':1,'New York Post':1,'Breitbart':1,\n",
    "              'Buzzfeed News':0,'Vox':0,'Atlantic':0,'Washington Post':0,'CNN':0}\n",
    "    \n",
    "    data = read_in_raw_data()\n",
    "    \n",
    "    \n",
    "    for heldout_outlet in outlets:\n",
    "        #tmp = [train_outlet for train_outlets in outlets if train_outlet != outlet]\n",
    "        train_data, test_data = get_test_train(heldout_outlet,data)\n",
    "        #model specific below\n",
    "        vectorizer, vocabulary, nmf = build_nmf(heldout_outlet,train_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "def remove_weak_outlets(valid_outlets,leanings,train_set,labels):\n",
    "    assert(len(train_set)==len(labels))\n",
    "    joined_data = zip(labels,train_set)\n",
    "    cleaned_data = []\n",
    "    cleaned_label = []\n",
    "    cleaned_outlet = []\n",
    "    #= pd.DataFrame([row[1] for row in joined_data if row[0] in valid_outlets])\n",
    "    for row in joined_data:\n",
    "        if row[0] in valid_outlets:\n",
    "            cleaned_data.append(row[1])\n",
    "            cleaned_outlet.append(row[0])\n",
    "            cleaned_label.append(leanings[row[0]])\n",
    "            #print('row0: ',row[0])\n",
    "            #print('leanings[row[0]]: ',leanings[row[0]])\n",
    "    return pd.DataFrame(cleaned_data), pd.Series(cleaned_outlet), pd.Series(cleaned_label)\n",
    "    #for label in labels:\n",
    "       # if label in valid_outlets\n",
    "\n",
    "        \n",
    "def test():\n",
    "    outlets =['Fox News','National Review','New York Post','Breitbart',\n",
    "              'Buzzfeed News','Vox','Atlantic','Washington Post','CNN']\n",
    "    leanings_dict = {'Fox News':1,'National Review':1,'New York Post':1,'Breitbart':1,\n",
    "              'Buzzfeed News':0,'Vox':0,'Atlantic':0,'Washington Post':0,'CNN':0}\n",
    "\n",
    "    data = read_in_raw_data()\n",
    "    for heldout_outlet in outlets: \n",
    "        #heldout_outlet = 'Atlantic'\n",
    "        #tmp = [train_outlet for train_outlets in outlets if train_outlet != outlet]\n",
    "        train_data, test_data = get_test_train(heldout_outlet,data)\n",
    "        #model specific below\n",
    "        vectorizer, vocabulary, nmf = build_nmf(heldout_outlet,train_data)\n",
    "        vectorized_train_data = vectorizer(train_data['content'])\n",
    "        components = nmf.transform(vectorized_train_data)\n",
    "\n",
    "        vectorized_train_data = vectorizer(train_data['content'])\n",
    "        components = nmf.transform(vectorized_train_data)\n",
    "        cleaned_train_data, cleaned_train_df, cleaned_train_y = remove_weak_outlets(outlets,leanings_dict,components,train_data['publication'])\n",
    "\n",
    "        test_data=test_data.reset_index(drop=True)\n",
    "        test_X = nmf.transform(vectorizer(test_data['content'])) #pd.Series([vectorizer(article) for article in test_data['content']])\n",
    "\n",
    "        pub_type = leanings_dict[test_data['publication'][0]]\n",
    "        test_y = pd.Series([pub_type for i in range(0,len(test_data))])\n",
    "        print('Score for: ', heldout_outlet)\n",
    "        print(rand_forest.score(test_X, test_y))\n",
    "    \n",
    "def test_grid_search():\n",
    "    trees = [100]\n",
    "    outlets =['Fox News','National Review','New York Post','Breitbart',\n",
    "                  'Buzzfeed News','Vox','Atlantic','Washington Post','CNN']\n",
    "    leanings_dict = {'Fox News':1,'National Review':1,'New York Post':1,'Breitbart':1,\n",
    "            'Buzzfeed News':0,'Vox':0,'Atlantic':0,'Washington Post':0,'CNN':0}\n",
    "\n",
    "    data = read_in_raw_data()\n",
    "    for heldout_outlet in outlets: \n",
    "        #heldout_outlet = 'Atlantic'\n",
    "        #tmp = [train_outlet for train_outlets in outlets if train_outlet != outlet]\n",
    "        train_data, test_data = get_test_train(heldout_outlet,data)\n",
    "        #model specific below\n",
    "        vectorizer, vocabulary, nmf = build_nmf(heldout_outlet,train_data)\n",
    "        vectorized_train_data = vectorizer(train_data['content'])\n",
    "        components = nmf.transform(vectorized_train_data)\n",
    "\n",
    "        #vectorized_train_data = vectorizer(train_data['content'])\n",
    "        #components = nmf.transform(vectorized_train_data)\n",
    "        cleaned_train_data, cleaned_train_df, cleaned_train_y = remove_weak_outlets(outlets,leanings_dict,components,train_data['publication'])\n",
    "\n",
    "        test_data=test_data.reset_index(drop=True)\n",
    "        test_X = nmf.transform(vectorizer(test_data['content'])) #pd.Series([vectorizer(article) for article in test_data['content']])\n",
    "\n",
    "        pub_type = leanings_dict[test_data['publication'][0]]\n",
    "        test_y = pd.Series([not pub_type for i in range(0,len(test_data))])\n",
    "        \n",
    "        for tree_count in trees:\n",
    "            rand_forest = train_random_forest(cleaned_train_data, _, cleaned_train_y, trees_in_forest = tree_count)\n",
    "            print('Score for: ', heldout_outlet, 'with ',tree_count, ' trees')\n",
    "\n",
    "            print(rand_forest.score(test_X, test_y))\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF as NMF_sklearn\n",
    "import pickle\n",
    "from nmf import NMF\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def build_text_vectorizer(contents, use_tfidf=True, use_stemmer=False, max_features=None):\n",
    "    '''\n",
    "    Build and return a **callable** for transforming text documents to vectors,\n",
    "    as well as a vocabulary to map document-vector indices to words from the\n",
    "    corpus. The vectorizer will be trained from the text documents in the\n",
    "    `contents` argument. If `use_tfidf` is True, then the vectorizer will use\n",
    "    the Tf-Idf algorithm, otherwise a Bag-of-Words vectorizer will be used.\n",
    "    The text will be tokenized by words, and each word will be stemmed iff\n",
    "    `use_stemmer` is True. If `max_features` is not None, then the vocabulary\n",
    "    will be limited to the `max_features` most common words in the corpus.\n",
    "    '''\n",
    "    \n",
    "    Vectorizer = TfidfVectorizer if use_tfidf else CountVectorizer\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']+\")\n",
    "    stem = PorterStemmer().stem if use_stemmer else (lambda x: x)\n",
    "    stop_set = set(stopwords.words('english'))\n",
    "\n",
    "    # Closure over the tokenizer et al.\n",
    "    def tokenize(text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        stems = [stem(token) for token in tokens if token not in stop_set]\n",
    "        return stems\n",
    "\n",
    "    vectorizer_model = Vectorizer(tokenizer=tokenize, max_features=max_features)\n",
    "    vectorizer_model.fit(contents)\n",
    "    vocabulary = np.array(vectorizer_model.get_feature_names())\n",
    "\n",
    "    # Closure over the vectorizer_model's transform method.\n",
    "    def vectorizer(X):\n",
    "        return vectorizer_model.transform(X).toarray()\n",
    "\n",
    "    return vectorizer, vocabulary\n",
    "\n",
    "\n",
    "def softmax(v, temperature=1.0):\n",
    "    '''\n",
    "    A heuristic to convert arbitrary positive values into probabilities.\n",
    "    See: https://en.wikipedia.org/wiki/Softmax_function\n",
    "    '''\n",
    "    expv = np.exp(v / temperature)\n",
    "    s = np.sum(expv)\n",
    "    return expv / s\n",
    "\n",
    "\n",
    "def hand_label_topics(H, vocabulary):\n",
    "    '''\n",
    "    Print the most influential words of each latent topic, and prompt the user\n",
    "    to label each topic. The user should use their humanness to figure out what\n",
    "    each latent topic is capturing.\n",
    "    '''\n",
    "    hand_labels = []\n",
    "    for i, row in enumerate(H):\n",
    "        top_five = np.argsort(row)[::-1][:20]\n",
    "        print('topic', i)\n",
    "        print('-->', ' '.join(vocabulary[top_five]))\n",
    "        label = input('please label this topic: ')\n",
    "        hand_labels.append(label)\n",
    "        print()\n",
    "    return hand_labels\n",
    "\n",
    "\n",
    "def analyze_article(article_index, contents, web_urls, W, hand_labels):\n",
    "    '''\n",
    "    Print an analysis of a single NYT articles, including the article text\n",
    "    and a summary of which topics it represents. The topics are identified\n",
    "    via the hand-labels which were assigned by the user.\n",
    "    '''\n",
    "    #print(web_urls[article_index])\n",
    "    #print(contents[article_index])\n",
    "    probs = softmax(W[article_index], temperature=0.01)\n",
    "    \n",
    "    top_prob = 0\n",
    "    top_cat = 0\n",
    "    i = 0\n",
    "    for prob, label in zip(probs, hand_labels):\n",
    "        if prob > top_prob:\n",
    "            top_prob = prob\n",
    "            top_cat = i\n",
    "        #print('--> {:.2f}% {}'.format(prob * 100, label))\n",
    "        i = i + 1\n",
    "    return top_cat, probs\n",
    "    #print()\n",
    "    #    gotta assign all of these to the correct bin and then tfidf them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_df1 = spark.read.csv('data/all-the-news/articles1.csv')\n",
    "g_df2 = spark.read.csv('data/all-the-news/articles2.csv')\n",
    "g_df3 = spark.read.csv('data/all-the-news/articles3.csv')\n",
    "merge1 = g_df1.union(g_df2)\n",
    "full_dataframe = merge1.union(g_df3)\n",
    "#g_arr1 = g_df1.values\n",
    "#g_arr2 = g_df2.values\n",
    "#g_arr3 = g_df3.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#fix\n",
    "#g_df1 = g_df_full #pd.read_csv('data/all-the-news/articles1.csv')\n",
    "\n",
    "g_df = g_df_full #pd.read_pickle(\"data/articles.pkl\")\n",
    "g_contents = g_df.content\n",
    "g_web_urls = g_df.url\n",
    "\n",
    "# Build our text-to-vector vectorizer, then vectorize our corpus.\n",
    "g_vectorizer, g_vocabulary = build_text_vectorizer(g_contents,\n",
    "                             use_tfidf=True,\n",
    "                             use_stemmer=False,\n",
    "                             max_features=5000)\n",
    "g_X = g_vectorizer(g_contents)\n",
    "\n",
    "# We'd like to see consistent results, so set the seed.\n",
    "np.random.seed(12345)\n",
    "\n",
    "g_rand_articles = list(range(len(g_df_full)))\n",
    "#for i in rand_articles:\n",
    "#        analyze_article(i, contents, web_urls, W, hand_labels)\n",
    "\n",
    "# Do it all again, this time using scikit-learn.\n",
    "g_nmf = NMF_sklearn(n_components=7, max_iter=100, random_state=12345, alpha=0.0)\n",
    "g_W = g_nmf.fit_transform(g_X)\n",
    "g_H = g_nmf.components_\n",
    "print('reconstruction error:', g_nmf.reconstruction_err_)\n",
    "g_hand_labels = ['Garbage', 'GOP', 'Intl Politics', 'Econ', 'Dems', 'Crime', 'Election'] #hand_label_topics(g_H, g_vocabulary)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#pickle.dump(H,open('nyt_clusters_H.p','wb'))\n",
    "#pickle.dump(W,open('nyt_clusters_W.p','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_df1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataframe.count9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

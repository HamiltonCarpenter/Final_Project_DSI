{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamiltoncarpenter/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF as NMF_sklearn\n",
    "import pickle\n",
    "from nmf import NMF\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pickle\n",
    "\n",
    "def build_text_vectorizer(contents, use_tfidf=True, use_stemmer=False, max_features=None):\n",
    "    '''\n",
    "    Build and return a **callable** for transforming text documents to vectors,\n",
    "    as well as a vocabulary to map document-vector indices to words from the\n",
    "    corpus. The vectorizer will be trained from the text documents in the\n",
    "    `contents` argument. If `use_tfidf` is True, then the vectorizer will use\n",
    "    the Tf-Idf algorithm, otherwise a Bag-of-Words vectorizer will be used.\n",
    "    The text will be tokenized by words, and each word will be stemmed iff\n",
    "    `use_stemmer` is True. If `max_features` is not None, then the vocabulary\n",
    "    will be limited to the `max_features` most common words in the corpus.\n",
    "    '''\n",
    "    \n",
    "    Vectorizer = TfidfVectorizer if use_tfidf else CountVectorizer\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']+\")\n",
    "    stem = PorterStemmer().stem if use_stemmer else (lambda x: x)\n",
    "    stop_set = set(stopwords.words('english'))\n",
    "\n",
    "    # Closure over the tokenizer et al.\n",
    "    def tokenize(text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        stems = [stem(token) for token in tokens if token not in stop_set]\n",
    "        return stems\n",
    "\n",
    "    vectorizer_model = Vectorizer(tokenizer=tokenize, max_features=max_features)\n",
    "    vectorizer_model.fit(contents)\n",
    "    vocabulary = np.array(vectorizer_model.get_feature_names())\n",
    "\n",
    "    # Closure over the vectorizer_model's transform method.\n",
    "    def vectorizer(X):\n",
    "        return vectorizer_model.transform(X).toarray()\n",
    "\n",
    "    return vectorizer, vocabulary\n",
    "\n",
    "\n",
    "def softmax(v, temperature=1.0):\n",
    "    '''\n",
    "    A heuristic to convert arbitrary positive values into probabilities.\n",
    "    See: https://en.wikipedia.org/wiki/Softmax_function\n",
    "    '''\n",
    "    expv = np.exp(v / temperature)\n",
    "    s = np.sum(expv)\n",
    "    return expv / s\n",
    "\n",
    "\n",
    "def hand_label_topics(H, vocabulary):\n",
    "    '''\n",
    "    Print the most influential words of each latent topic, and prompt the user\n",
    "    to label each topic. The user should use their humanness to figure out what\n",
    "    each latent topic is capturing.\n",
    "    '''\n",
    "    hand_labels = []\n",
    "    for i, row in enumerate(H):\n",
    "        top_five = np.argsort(row)[::-1][:20]\n",
    "        print('topic', i)\n",
    "        print('-->', ' '.join(vocabulary[top_five]))\n",
    "        label = input('please label this topic: ')\n",
    "        hand_labels.append(label)\n",
    "        print()\n",
    "    return hand_labels\n",
    "\n",
    "\n",
    "def analyze_article(article_index, contents, web_urls, W, hand_labels):\n",
    "    '''\n",
    "    Print an analysis of a single NYT articles, including the article text\n",
    "    and a summary of which topics it represents. The topics are identified\n",
    "    via the hand-labels which were assigned by the user.\n",
    "    '''\n",
    "    probs = softmax(W[article_index], temperature=0.01)\n",
    "    \n",
    "    top_prob = 0\n",
    "    top_cat = 0\n",
    "    i = 0\n",
    "    for prob, label in zip(probs, hand_labels):\n",
    "        if prob > top_prob:\n",
    "            top_prob = prob\n",
    "            top_cat = i\n",
    "        i = i + 1\n",
    "    return top_cat, probs\n",
    "\n",
    "\n",
    "\n",
    "def build_nmf(heldout_name,train_data):\n",
    "    #make_names:\n",
    "    vectorizer_file_name = 'cache/'+heldout_name+' vectorizer'\n",
    "    vocabulary_file_name = 'cache/'+heldout_name+' vocabulary'\n",
    "    nmf_file_name = 'cache/'+heldout_name+' nmf'\n",
    "    \n",
    "    g_df = train_data \n",
    "    g_contents = g_df.content\n",
    "    g_web_urls = g_df.url\n",
    "\n",
    "    # Build our text-to-vector vectorizer, then vectorize our corpus.\n",
    "    g_vectorizer, g_vocabulary = build_text_vectorizer(g_contents,\n",
    "                                 use_tfidf=True,\n",
    "                                 use_stemmer=False,\n",
    "                                 max_features=5000)\n",
    "    g_X = g_vectorizer(g_contents)\n",
    "\n",
    "    # We'd like to see consistent results, so set the seed.\n",
    "    np.random.seed(12345)\n",
    "\n",
    "    # Do it all again, this time using scikit-learn.\n",
    "    g_nmf = NMF_sklearn(n_components=6, max_iter=100, random_state=12345, alpha=0.0)\n",
    "    g_W = g_nmf.fit_transform(g_X)\n",
    "    g_H = g_nmf.components_\n",
    "    print('reconstruction error:', g_nmf.reconstruction_err_)\n",
    "    #g_hand_labels = ['label_one', 'label_two', 'label_three', 'label_four', 'label_five', 'label_one'] # hand_label_topics(g_H, g_vocabulary) #\n",
    "    \n",
    "    return g_vectorizer, g_vocabulary, g_nmf\n",
    "\n",
    "def train_random_forest(cleaned_nmf_compenents, raw_training_data, cleaned_train_y,trees_in_forest = 100):\n",
    "    print('trees in this forest: ', trees_in_forest)\n",
    "    y_train = cleaned_train_y\n",
    "    X_train = cleaned_nmf_compenents\n",
    "    rf = RandomForestClassifier(n_estimators=trees_in_forest, oob_score=True)\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf\n",
    "\n",
    "def read_in_raw_data():\n",
    "    g_df1 = pd.read_csv('data/all-the-news/articles1.csv')\n",
    "    g_df2 = pd.read_csv('data/all-the-news/articles2.csv')\n",
    "    g_df3 = pd.read_csv('data/all-the-news/articles3.csv')\n",
    "\n",
    "    g_arr1 = g_df1.values\n",
    "    g_arr2 = g_df2.values\n",
    "    g_arr3 = g_df3.values\n",
    "\n",
    "    g_df_full = g_df1.append(g_df2).append(g_df3)\n",
    "    g_df_reset=g_df_full.reset_index(drop=True)\n",
    "    g_df_reset.head()\n",
    "    g_df_reset.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    g_df_full = g_df_reset\n",
    "    return g_df_full\n",
    "    \n",
    "def get_test_train(heldout,data):\n",
    "    return data[(data['publication'] != heldout)], data[(data['publication'] == heldout)]\n",
    "    \n",
    "def evaluate_model(vectorizer, vocabulary, nmf, heldout_outlet, test_data,outlets):\n",
    "    train_data, test_data = get_test_train(heldout,data)\n",
    "    \n",
    "    \n",
    "#maybe remove wapo\n",
    "def kfolds():\n",
    "    outlets =['Fox News','National Review','National Review','New York Post','Breitbart',\n",
    "              'Buzzfeed News','Vox','Atlantic','Washington Post','CNN']\n",
    "    leanings_dict = {'Fox News':1,'National Review':1,'National Review':1,'New York Post':1,'Breitbart':1,\n",
    "              'Buzzfeed News':0,'Vox':0,'Atlantic':0,'Washington Post':0,'CNN':0}\n",
    "    data = read_in_raw_data()\n",
    "    \n",
    "    for heldout_outlet in outlets:\n",
    "        #tmp = [train_outlet for train_outlets in outlets if train_outlet != outlet]\n",
    "        train_data, test_data = get_test_train(heldout_outlet,data)\n",
    "        vectorizer, vocabulary, nmf = build_nmf(heldout_outlet,train_data)\n",
    "        \n",
    "def remove_weak_outlets(valid_outlets,leanings,train_set,labels):\n",
    "    assert(len(train_set)==len(labels))\n",
    "    joined_data = zip(labels,train_set)\n",
    "    cleaned_data = []\n",
    "    cleaned_label = []\n",
    "    cleaned_outlet = []\n",
    "    #= pd.DataFrame([row[1] for row in joined_data if row[0] in valid_outlets])\n",
    "    for row in joined_data:\n",
    "        if row[0] in valid_outlets:\n",
    "            cleaned_data.append(row[1])\n",
    "            cleaned_outlet.append(row[0])\n",
    "            cleaned_label.append(leanings[row[0]])\n",
    "            #print('row0: ',row[0])\n",
    "            #print('leanings[row[0]]: ',leanings[row[0]])\n",
    "    return pd.DataFrame(cleaned_data), pd.Series(cleaned_outlet), pd.Series(cleaned_label)\n",
    "        \n",
    "#def test():\n",
    "#    outlets =['Fox News','National Review','New York Post','Breitbart',\n",
    "#              'Buzzfeed News','Vox','Atlantic','Washington Post','CNN']\n",
    "#    leanings_dict = {'Fox News':1,'National Review':1,'New York Post':1,'Breitbart':1,\n",
    "#              'Buzzfeed News':0,'Vox':0,'Atlantic':0,'Washington Post':0,'CNN':0}\n",
    "\n",
    "#    data = read_in_raw_data()\n",
    "#    for heldout_outlet in outlets: \n",
    "#        #heldout_outlet = 'Atlantic'\n",
    "        #tmp = [train_outlet for train_outlets in outlets if train_outlet != outlet]\n",
    "#        train_data, test_data = get_test_train(heldout_outlet,data)\n",
    "        #model specific below\n",
    "#        vectorizer, vocabulary, nmf = build_nmf(heldout_outlet,train_data)\n",
    "#        vectorized_train_data = vectorizer(train_data['content'])\n",
    "#        components = nmf.transform(vectorized_train_data)\n",
    "\n",
    "#        vectorized_train_data = vectorizer(train_data['content'])\n",
    "#        components = nmf.transform(vectorized_train_data)\n",
    "#        cleaned_train_data, cleaned_train_df, cleaned_train_y = remove_weak_outlets(outlets,leanings_dict,components,train_data['publication'])\n",
    "\n",
    "#        test_data=test_data.reset_index(drop=True)\n",
    "#        test_X = nmf.transform(vectorizer(test_data['content'])) #pd.Series([vectorizer(article) for article in test_data['content']])\n",
    "\n",
    "#        pub_type = leanings_dict[test_data['publication'][0]]\n",
    "#        test_y = pd.Series([pub_type for i in range(0,len(test_data))])\n",
    "#        print('Score for: ', heldout_outlet)\n",
    "#        print(rand_forest.score(test_X, test_y))\n",
    "    \n",
    "def test_grid_search():\n",
    "    trees = [100]\n",
    "    outlets =['Fox News','National Review','New York Post','Breitbart',\n",
    "                  'Buzzfeed News','Vox','Atlantic','Washington Post','CNN']\n",
    "    leanings_dict = {'Fox News':1,'National Review':1,'New York Post':1,'Breitbart':1,\n",
    "            'Buzzfeed News':0,'Vox':0,'Atlantic':0,'Washington Post':0,'CNN':0}\n",
    "\n",
    "    data = read_in_raw_data()\n",
    "    for heldout_outlet in outlets: \n",
    "        #heldout_outlet = 'Atlantic'\n",
    "        #tmp = [train_outlet for train_outlets in outlets if train_outlet != outlet]\n",
    "        \n",
    "        train_data, test_data = get_test_train(heldout_outlet,data)\n",
    "        vectorizer, vocabulary, nmf = build_nmf(heldout_outlet,train_data)\n",
    "        vectorized_train_data = vectorizer(train_data['content'])\n",
    "        components = nmf.transform(vectorized_train_data)\n",
    "        \n",
    "        cleaned_train_data, cleaned_train_df, cleaned_train_y = remove_weak_outlets(outlets,leanings_dict,components,train_data['publication'])\n",
    "        test_data=test_data.reset_index(drop=True)\n",
    "        test_X = nmf.transform(vectorizer(test_data['content'])) #pd.Series([vectorizer(article) for article in test_data['content']])\n",
    "        pub_type = leanings_dict[test_data['publication'][0]]\n",
    "        test_y = pd.Series([not pub_type for i in range(0,len(test_data))])\n",
    "        \n",
    "        for tree_count in trees:\n",
    "            rand_forest = train_random_forest(cleaned_train_data, _, cleaned_train_y, trees_in_forest = tree_count)\n",
    "            print('Score for: ', heldout_outlet, 'with ',tree_count, ' trees')\n",
    "\n",
    "            print(rand_forest.f1_score(test_X, test_y))\n",
    "            \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction error: 354.3197000648747\n",
      "trees in this forest:  100\n",
      "Score for:  Fox News with  100  trees\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassifier' object has no attribute 'f1_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7e0330018b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-7e69489bce0b>\u001b[0m in \u001b[0;36mtest_grid_search\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Score for: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheldout_outlet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'with '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtree_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' trees'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'f1_score'"
     ]
    }
   ],
   "source": [
    "test_grid_search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF as NMF_sklearn\n",
    "import pickle\n",
    "from nmf import NMF\n",
    "\n",
    "\n",
    "def build_text_vectorizer(contents, use_tfidf=True, use_stemmer=False, max_features=None):\n",
    "    '''\n",
    "    Build and return a **callable** for transforming text documents to vectors,\n",
    "    as well as a vocabulary to map document-vector indices to words from the\n",
    "    corpus. The vectorizer will be trained from the text documents in the\n",
    "    `contents` argument. If `use_tfidf` is True, then the vectorizer will use\n",
    "    the Tf-Idf algorithm, otherwise a Bag-of-Words vectorizer will be used.\n",
    "    The text will be tokenized by words, and each word will be stemmed iff\n",
    "    `use_stemmer` is True. If `max_features` is not None, then the vocabulary\n",
    "    will be limited to the `max_features` most common words in the corpus.\n",
    "    '''\n",
    "    \n",
    "    Vectorizer = TfidfVectorizer if use_tfidf else CountVectorizer\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']+\")\n",
    "    stem = PorterStemmer().stem if use_stemmer else (lambda x: x)\n",
    "    stop_set = set(stopwords.words('english'))\n",
    "\n",
    "    # Closure over the tokenizer et al.\n",
    "    def tokenize(text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        stems = [stem(token) for token in tokens if token not in stop_set]\n",
    "        return stems\n",
    "\n",
    "    vectorizer_model = Vectorizer(tokenizer=tokenize, max_features=max_features)\n",
    "    vectorizer_model.fit(contents)\n",
    "    vocabulary = np.array(vectorizer_model.get_feature_names())\n",
    "\n",
    "    # Closure over the vectorizer_model's transform method.\n",
    "    def vectorizer(X):\n",
    "        return vectorizer_model.transform(X).toarray()\n",
    "\n",
    "    return vectorizer, vocabulary\n",
    "\n",
    "\n",
    "def softmax(v, temperature=1.0):\n",
    "    '''\n",
    "    A heuristic to convert arbitrary positive values into probabilities.\n",
    "    See: https://en.wikipedia.org/wiki/Softmax_function\n",
    "    '''\n",
    "    expv = np.exp(v / temperature)\n",
    "    s = np.sum(expv)\n",
    "    return expv / s\n",
    "\n",
    "\n",
    "def hand_label_topics(H, vocabulary):\n",
    "    '''\n",
    "    Print the most influential words of each latent topic, and prompt the user\n",
    "    to label each topic. The user should use their humanness to figure out what\n",
    "    each latent topic is capturing.\n",
    "    '''\n",
    "    hand_labels = []\n",
    "    for i, row in enumerate(H):\n",
    "        top_five = np.argsort(row)[::-1][:20]\n",
    "        print('topic', i)\n",
    "        print('-->', ' '.join(vocabulary[top_five]))\n",
    "        label = input('please label this topic: ')\n",
    "        hand_labels.append(label)\n",
    "        print()\n",
    "    return hand_labels\n",
    "\n",
    "\n",
    "def analyze_article(article_index, contents, web_urls, W, hand_labels):\n",
    "    '''\n",
    "    Print an analysis of a single NYT articles, including the article text\n",
    "    and a summary of which topics it represents. The topics are identified\n",
    "    via the hand-labels which were assigned by the user.\n",
    "    '''\n",
    "    print(web_urls[article_index])\n",
    "    print(contents[article_index])\n",
    "    probs = softmax(W[article_index], temperature=0.01)\n",
    "    \n",
    "    top_prob = 0\n",
    "    top_cat = 0\n",
    "    i = 0\n",
    "    for prob, label in zip(probs, hand_labels):\n",
    "        if prob > top_prob:\n",
    "            top_prob = prob\n",
    "            top_cat = i\n",
    "        #print('--> {:.2f}% {}'.format(prob * 100, label))\n",
    "        i = i + 1\n",
    "    return top_cat\n",
    "    #print()\n",
    "    #    gotta assign all of these to the correct bin and then tfidf them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run the unsupervised analysis of the NYT corpus, using NMF to find latent\n",
    "topics. The user will be prompted to label each latent topic, then a few\n",
    "articles will be analyzed to see which topics they contain.\n",
    "'''\n",
    "# Load the corpus.\n",
    "df = pd.read_pickle(\"data/articles.pkl\")\n",
    "contents = df.content\n",
    "web_urls = df.web_url\n",
    "\n",
    "# Build our text-to-vector vectorizer, then vectorize our corpus.\n",
    "vectorizer, vocabulary = build_text_vectorizer(contents,\n",
    "                             use_tfidf=True,\n",
    "                             use_stemmer=False,\n",
    "                             max_features=5000)\n",
    "X = vectorizer(contents)\n",
    "\n",
    "# We'd like to see consistent results, so set the seed.\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Label topics and analyze a few NYT articles.\n",
    "# Btw, if you haven't modified anything, the seven topics which should\n",
    "# pop out are:  (you should type these as the labels when prompted)\n",
    "#  1. \"football\",\n",
    "#  2. \"arts\",\n",
    "#  3. \"baseball\",\n",
    "#  4. \"world news (middle eastern?)\",\n",
    "#  5. \"politics\",\n",
    "#  6. \"world news (war?)\",\n",
    "#  7. \"economics\"\n",
    "#    hand_labels = hand_label_topics(H, vocabulary)\n",
    "rand_articles = np.random.choice(list(range(len(W))), 15)\n",
    "#for i in rand_articles:\n",
    "#        analyze_article(i, contents, web_urls, W, hand_labels)\n",
    "\n",
    "# Do it all again, this time using scikit-learn.\n",
    "nmf = NMF_sklearn(n_components=7, max_iter=100, random_state=12345, alpha=0.0)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "print('reconstruction error:', nmf.reconstruction_err_)\n",
    "hand_labels = hand_label_topics(H, vocabulary)\n",
    "for i in range(0,len(rand_articles)):\n",
    "    clus = analyze_article(i, contents, web_urls, W, hand_labels)\n",
    "\n",
    "    cl = MongoClient()\n",
    "    coll = cl[\"nyt_clustering\"][\"clustered_articles\"]\n",
    "\n",
    "    data = {\"_id\" : i, \"content\" : contents[i], \"url\" : web_urls[i], \"cluster\" : clus}\n",
    "    coll.insert({'_id':d['_id']}, d, True)\n",
    "        \n",
    "pickle.dump(H,open('nyt_clusters_H.p','wb'))\n",
    "pickle.dump(W,open('nyt_clusters_W.p','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=100,\n",
       "  n_components=7, random_state=12345, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 482, 1309,  129,  382, 1381,  546,  769, 1142, 1393, 1339,  654,\n",
       "        105, 1115,  759, 1398])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(list(range(len(W))), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1405, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(0,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    http://www.nytimes.com/2013/10/03/sports/footb...\n",
       "1    http://www.nytimes.com/2013/10/03/us/new-immig...\n",
       "2    http://www.nytimes.com/2013/10/03/us/arizona-j...\n",
       "3    http://www.nytimes.com/2013/10/03/us/texas-sta...\n",
       "4    http://www.nytimes.com/2013/10/03/sports/tenni...\n",
       "Name: web_url, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rand_articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-054762f06474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrand_articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rand_articles' is not defined"
     ]
    }
   ],
   "source": [
    "rand_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
